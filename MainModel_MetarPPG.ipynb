{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da2f3a8",
   "metadata": {},
   "source": [
    "This section implements the overall Meta rPPG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0894592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general libraries\n",
    "import ipynb\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "from scipy import signal\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573651ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes created in other files\n",
    "from ipynb.fs.full.Submodel_MetarPPG import Convolutional_Encoder, rPPG_Estimator, Synthetic_Gradient_Generator\n",
    "from ipynb.fs.full.Loss_MetarPPG import ordLoss\n",
    "from ipynb.fs.full.Data_MetarPPG import butter_bandpass_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca979d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_rppg(nn.Module):\n",
    "    def _init_(self, opt, isTrain, continue_train=False, norm_layer=nn.BatchNorm2d):\n",
    "        # Supercharge the meta_rppg class so that it can inherit from itself\n",
    "        super(meta_rppg, self)._init_()\n",
    "        \n",
    "        # Save the directory\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.opt = opt\n",
    "        self.isTrain = isTrain\n",
    "        self.continue_train = continue_train\n",
    "        self.threshold = 0.5\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        if self.gpu_ids:\n",
    "            self.device = torch.device('cuda:{}'.format(self.gpu_ids[0]))\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        self.prototype = torch.zeros(120)\n",
    "        self.h = torch.zeros(2*opt.lstm_num_layers, opt.batch_size, 60).to(self.device)\n",
    "        self.c = torch.zeros(2*opt.lstm_num_layers, opt.batch_size, 60).to(self.device)\n",
    "        \n",
    "        # Initializing the three sub models\n",
    "        self.con = Convolutional_Encoder(no_input_channel=3, isTrain=self.isTrain, device=self.device)\n",
    "        self.rppg = rPPG_Estimator(no_input_channel=120, num_layers=opt.lstm_num_layers, isTrain=self.isTrain, device=self.device, h=self.h, c=self.c)\n",
    "        self.syn = Synthetic_Gradient_Generator(number_input_channels=120, isTrain=self.isTrain, device=self.device)\n",
    "        \n",
    "        # Set sub models state to device\n",
    "        self.con.to(self.device)\n",
    "        self.rppg.to(self.device)\n",
    "        self.syn.to(self.device)\n",
    "        \n",
    "        # Define the overall model\n",
    "        self.model = [self.con, self.rppg.to(self.device), self.syn.to(self.device)]\n",
    "        \n",
    "        # Initialize the loss variables\n",
    "        self.few_shotloss = 0.0\n",
    "        self.ordloss = 0.0\n",
    "        self.gradientloss = 0.0\n",
    "        \n",
    "        # Set the loss criterions\n",
    "        self.criterion1 = torch.nn.MSELoss()\n",
    "        self.criterion2 = ordLoss()\n",
    "        self.criterion3 = torch.nn.MSELoss()\n",
    "        \n",
    "        \n",
    "        momentum = 0.9\n",
    "        weight_decay = 5e-4\n",
    "        # Set optimizers for each individual sub model\n",
    "        self.optimizer_con = torch.optim.SGD(self.con.parameters(), opt.lr, momentum, weight_decay)\n",
    "        self.optimizer_rppg = torch.optim.SGD(self.rppg.parameters(), opt.lr, momentum, weight_decay)\n",
    "        self.optimizer_syn = torch.optim.SGD(self.syn.parameters(), opt.lr, momentum, weight_decay)\n",
    "        \n",
    "        # set the optimization for the psi variable which is the hyperparameter for the update of our meta-learning system\n",
    "        if self.opt.adapt_position == 'extractor':\n",
    "            self.optimizer_psi = torch.optim.SGD(self.con.parameters(), opt.lr*1e-2, momentum, weight_decay)\n",
    "        elif self.opt.adapt_position == 'estimator':\n",
    "            self.optimizer_psi = torch.optim.SGD(self.rppg.parameters(), opt.lr*1e-2, momentum, weight_decay)\n",
    "        elif self.opt.adapt_position == 'both':\n",
    "            self.optimizer_psi = torch.optim.SGD(itertools.chain(self.con.parameters(), self.rppg.parameters()), opt.lr*1e-2, momentum, weight_decay)\n",
    "        \n",
    "        # Set the schedulers for each sub model and hyperparameter\n",
    "        self.scheduler_con = optim.lr_scheduler.CosineAnnealingLR(self.optimizer_con, T_max=5, eta_min=0.1*opt.lr)\n",
    "        self.scheduler_rppg = optim.lr_scheduler.CosineAnnealingLR(self.optimizer_rppg, T_max=5, eta_min=0.1*opt.lr)\n",
    "        self.scheduler_syn = optim.lr_scheduler.CosineAnnealingLR(self.optimizer_syn, T_max=5, eta_min=0.1*opt.lr)\n",
    "        self.scheduler_psi = optim.lr_scheduler.CosineAnnealingLR(self.optimizer_psi, T_max=5, eta_min=0.1*1e-2*opt.lr)\n",
    "        \n",
    "    \n",
    "    # Show the number of parameters in the system\n",
    "    def print_networks(self, print_net):\n",
    "        print('----------- Networks initialized -------------')\n",
    "        num = 0\n",
    "        for param in self.con.parameters():\n",
    "            num += param.numel()\n",
    "        for param in self.rppg.parameters():\n",
    "            num += param.numel()\n",
    "        for param in self.syn.parameters():\n",
    "            num += param.numel()\n",
    "        if print_net:\n",
    "            print(self.model)\n",
    "        print('Total number of parameters : %.3f M' % (num/1e6))\n",
    "        print('----------------------------------------------')\n",
    "        \n",
    "    \n",
    "    # Set the inputs\n",
    "    def set_input(self, input):\n",
    "        self.input = input['input']\n",
    "        self.rPPG = input['rPPG']\n",
    "        self.center = input['center']\n",
    "        \n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward(self, x):\n",
    "        self.inter = self.con(x)\n",
    "        # find condition and estimation\n",
    "        self.condition, self.estimate = self.rppg(self.inter)\n",
    "        # Set the gradient variable based on phase \n",
    "        if self.opt.adapt_position == 'extractor':\n",
    "            self.gradient = self.syn(self.inter.detach())\n",
    "        elif self.opt.adapt_position == 'estimator':\n",
    "            self.gradient = self.syn(self.predict.detach())\n",
    "        elif self.opt.adapt_position == 'both':\n",
    "            self.gradient1 = self.syn(self.inter.detach())\n",
    "            self.gradient2 = self.syn(self.predict.detach())\n",
    "            \n",
    "    \n",
    "    # update theta - feature extractor variable\n",
    "    def theta_update(self, epoch):\n",
    "        inter = self.con(self.input.to(self.device))\n",
    "        condition, estimate = self.rppg(inter)\n",
    "        \n",
    "        # initialize the loss\n",
    "        few_shotloss = self.criterion1(self.prototype.expand(self.opt.batch_size, 60, 120), inter)\n",
    "        ordloss = self.criterion2(estimate, self.rPPG.to(self.device))\n",
    "        \n",
    "        self.optimizer_con.zero_grad()\n",
    "        loss = few_shotloss + ordloss\n",
    "        loss.backward()\n",
    "        self.optimizer_con.step()\n",
    "        \n",
    "        # update weight based on current phase\n",
    "        if self.opt.adapt_position == \"extractor\":\n",
    "            for i in range(self.opt.fewshots):\n",
    "                inter = self.con(self.input.to(self.device))\n",
    "                condition, estimate = self.rppg(inter)\n",
    "                inter_grad = self.syn(inter.detach())\n",
    "                self.optimizer_psi.zero_grad()\n",
    "                grad = torch.autograd.grad(outputs=inter, inputs=self.con.parameters(), grad_outputs=inter_grad, create_graph=False, retain_graph=False)\n",
    "                torch.autograd.backward(self.con.parameters(), grad_tensors=grad, retain_graph=False, create_graph=False)\n",
    "                self.optimizer_psi.step()\n",
    "            self.gradient = inter_grad.detach().clone()\n",
    "        elif self.opt.adapt_position == \"estimator\":\n",
    "            for i in range(self.opt.fewshots):\n",
    "                inter = self.con(self.input.to(self.device))\n",
    "                condition, estimate = self.rppg(inter)\n",
    "                predict_grad = self.syn(predict.detach())\n",
    "                self.optimizer_psi.zero_grad()\n",
    "                grad = torch.autograd.grad(outputs=predict, inputs=self.rppg.parameters(), grad_outputs=predict_grad, create_graph=False, retain_graph=False)\n",
    "                torch.autograd.backward(self.rppg.parameters(), grad_tensors=grad, retain_graph=False, create_graph=False)\n",
    "                self.optimizer_psi.step()\n",
    "            self.gradient = predict_grad.detach().clone()\n",
    "        elif self.opt.adapt_position == \"both\":\n",
    "            for i in range(self.opt.fewshots):\n",
    "                inter = self.con(self.input.to(self.device))\n",
    "                condition, estimate = self.rppg(inter)\n",
    "                inter_grad = self.syn(inter.detach())\n",
    "                predict_grad = self.syn(predict.detach())\n",
    "                self.optimizer_psi.zero_grad()\n",
    "                grad = torch.autograd.grad(outputs=inter, inputs=self.con.parameters(), grad_outputs=inter_grad, create_graph=False, retain_graph=False)\n",
    "                torch.autograd.backward(self.con.parameters(), grad_tensors=grad, retain_graph=False, create_graph=False)\n",
    "                grad = torch.autograd.grad(outputs=predict, inputs=self.rppg.parameters(), grad_outputs=predict_grad, create_graph=False, retain_graph=False)\n",
    "                torch.autograd.backward(self.rppg.parameters(), grad_tensors=grad, retain_graph=False, create_graph=False)\n",
    "                self.optimizer_psi.step()\n",
    "            self.gradient = predict_grad.detach().clone()\n",
    "        \n",
    "        # output the variables\n",
    "        self.few_shotloss = few_shotloss.detach().clone()\n",
    "        self.ordloss = ordloss.detach().clone()\n",
    "        self.inter = inter.detach().clone()\n",
    "        \n",
    "        \n",
    "    # Similarly update psi and phi based on current phase\n",
    "    def psi_phi_update(self, epoch):\n",
    "        if self.opt.adapt_position == \"extractor\":\n",
    "            inter = self.con(self.input.to(self.device))\n",
    "            condition, estimate = self.rppg(inter)\n",
    "            inter_grad = self.syn(inter.detach())\n",
    "            \n",
    "            inter.retain_grad()\n",
    "            few_shotloss = self.criterion1(self.prototype.expand(self.opt.batch_size, 60, 120), inter)\n",
    "            ordloss = self.criterion2(estimate, self.rPPG.to(self.device))\n",
    "            loss = few_shotloss + ordloss\n",
    "            \n",
    "            self.optimizer_con.zero_grad()\n",
    "            self.optimizer_rppg.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_con.step()\n",
    "            self.optimizer_rppg.step()\n",
    "            \n",
    "            gradloss = self.criterion3(inter_grad, inter.grad)\n",
    "            self.optimizer_syn.zero_grad()\n",
    "            gradloss.backward()\n",
    "            self.optimizer_syn.step()\n",
    "            self.gradloss = gradloss.detach().clone()\n",
    "            \n",
    "        elif self.opt.adapt_position == \"estimator\":\n",
    "            inter = self.con(self.input.to(self.device))\n",
    "            condition, estimate = self.rppg(inter)\n",
    "            predict_grad = self.syn(predict.detach())\n",
    "            \n",
    "            predict.retain_grad()\n",
    "            few_shotloss = self.criterion1(self.prototype.expand(self.opt.batch_size, 60, 120), inter)\n",
    "            ordloss = self.criterion2(estimate, self.rPPG.to(self.device))\n",
    "            loss = few_shotloss + ordloss\n",
    "            \n",
    "            self.optimizer_con.zero_grad()\n",
    "            self.optimizer_rppg.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_con.step()\n",
    "            self.optimizer_rppg.step()\n",
    "            \n",
    "            gradloss = self.criterion3(predict_grad, predict.grad)\n",
    "            self.optimizer_syn.zero_grad()\n",
    "            gradloss.backward()\n",
    "            self.optimizer_syn.step()\n",
    "            self.gradloss = gradloss.detach().clone()\n",
    "            \n",
    "        elif self.opt.adapt_position == \"both\":\n",
    "            inter = self.con(self.input.to(self.device))\n",
    "            condition, estimate = self.rppg(inter)\n",
    "            inter_grad = self.syn(inter.detach())\n",
    "            predict_grad = self.syn(predict.detach())\n",
    "            \n",
    "            inter.retain_grad()\n",
    "            predict.ratin_grad()\n",
    "            few_shotloss = self.criterion1(self.prototype.expand(self.opt.batch_size, 60, 120), inter)\n",
    "            ordloss = self.criterion2(estimate, self.rPPG.to(self.device))\n",
    "            loss = few_shotloss + ordloss\n",
    "            \n",
    "            self.optimizer_con.zero_grad()\n",
    "            self.optimizer_rppg.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_con.step()\n",
    "            self.optimizer_rppg.step()\n",
    "            \n",
    "            gradloss = self.criterion3(inter_grad, inter.grad) + self.criterion3(predict_grad, predict.grad)\n",
    "            self.optimizer_syn.zero_grad()\n",
    "            gradloss.backward()\n",
    "            self.optimizer_syn.step()\n",
    "            self.gradloss = gradloss.detach().clone()\n",
    "            \n",
    "        # output the variables\n",
    "        self.condition = condition.detach().clone()\n",
    "        self.estimate = estimate.detach().clone()\n",
    "        self.ordloss = ordloss.detach().clone()\n",
    "        \n",
    "    # Prototypical distance\n",
    "    def update_prototype(self):\n",
    "        proto = torch.zeros(120).to(self.device)\n",
    "        h_tmp = torch.zeros(2*opt.lstm_num_layers, opt.batch_size, 60).to(self.device)\n",
    "        c_tmp = torch.zeros(2*opt.lstm_num_layers, opt.batch_size, 60).to(self.device)\n",
    "        self.rppg.feed_hc([self.h, self.c])\n",
    "        \n",
    "        self.forward(self.input.to(self.device))\n",
    "        proto += self.inter.data.mean(axis=[0,1])\n",
    "        h_tmp += self.rppg.h.data\n",
    "        c_tmp += self.rppg.c.data\n",
    "        \n",
    "        # update the prototypical distance for first update and then every other update\n",
    "        if torch.sum(self.prototype) == 0:\n",
    "            self.prototype = proto\n",
    "            (self.h, self.c) = (h_tmp, c_tmp)\n",
    "        else:\n",
    "            self.prototype = 0.8*self.prototype + 0.2*proto\n",
    "            (self.h, self.c) = (0.8*self.h + 0.2*h_tmp, 0.8*self.c + 0.2*c_tmp)\n",
    "            \n",
    "    # Initialize weights\n",
    "    def init_weights(net1, net2, init_type='normal', init_gain=0.02):\n",
    "        net1.apply(init_func)\n",
    "        net2.apply(init_func)\n",
    "        \n",
    "        \n",
    "    # Setup network\n",
    "    def setup(self, opt):\n",
    "        self.init_weights(self.con, self.rppg)\n",
    "        if self.continue_train:\n",
    "            self.load_networks(opt.load_file)\n",
    "            self.thres = 0.01\n",
    "        if not self.isTrain:\n",
    "            self.load_networks(opt.load_file)\n",
    "        self.print_networks(opt.print_net)\n",
    "        \n",
    "    \n",
    "    # Save network to disc - good practice in case of any accidents while running\n",
    "    def save(self, suffix):\n",
    "        save_filename = '%s_%s.pth' % (suffix, self.opt.name)\n",
    "        save_path = os.path.join(self.save_dir, save_filename)\n",
    "        torch.save({'Con': self.con.state_dict(), 'rPPG': self.rppg.state_dict(), 'SGG': self.syn.state_dict(),\n",
    "                    'proto': self.prototype.cpu(), 'h': self.h.data.cpu(), 'c': self.c.data.cpu()},\n",
    "                    save_path1)\n",
    "    \n",
    "    def get_current_results(self, isTest):\n",
    "        return self.condition[-1].cpu().clone(), self.rPPG[-1].cpu().clone()\n",
    "                \n",
    "    def get_current_losses(self, isTest):\n",
    "        if isTest:\n",
    "            return self.new_ordless\n",
    "        else:\n",
    "            return [self.few_shotloss, self.gradloss, self.ordloss]\n",
    "    \n",
    "    def train(self):\n",
    "        self.con.train()\n",
    "        self.rppg.train()\n",
    "        self.syn.train()\n",
    "    \n",
    "    def few_shotloss_test(self, epoch):\n",
    "        momentum=0.9\n",
    "        weight_decay=5e-4\n",
    "        conv = pickle.loads(pickle.dumps(self.con))\n",
    "        optim = torch.optim.SGD(A.parameters(), self.opt.lr*1e-2, momentum, weight_decay)\n",
    "        \n",
    "        for i in range(self.opt.few_shots):\n",
    "            optim.zero_grad()\n",
    "            inter = conv(self.input[i].unsqueeze(0).to(self.device))\n",
    "            inter_grad = self.syn(inter)\n",
    "            grad = torch.autograd.grad(outputs=inter, inputs=conv.parameters(), grad_outputs=inter_grad, create_graph=False, retain_graph=False)\n",
    "            torch.autograd.backward(conv.parameters(), grad_tensors=grad, retain_graph=False, create_graph=False)\n",
    "            optim.step()\n",
    "        for i in range(self.opt.few_shots):\n",
    "            optim.zero_grad()\n",
    "            inter = conv(self.input[i].unsqueeze(0).to(self.device))\n",
    "            loss = self.criterion1(inter, self.prototype.expand(1, 60, 120))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        with torch.no_grad():\n",
    "            tmp_h = self.rppg.h\n",
    "            tmp_c = self.rppg.c\n",
    "            self.rppg.feed_hc([self.h, self.c])\n",
    "            data = self.input[self.opt.fewshots:]\n",
    "            inter = conv(data.to(self.device))\n",
    "            self.decision, self.predict = self.rppg(inter)\n",
    "            self.rppg.feed_hc([tmp_h, tmp_c])\n",
    "            \n",
    "        self.new_ordless = self.criterion2(self.predict[0].unsqueeze(0), self.rPPG[0].unsqueeze(0).to(self.device))\n",
    "        \n",
    "    \n",
    "    # Load network from disk\n",
    "    def load_networks(self, suffix):\n",
    "        load_filename = '%s_%s.pth' % (suffix, self.opt.name)\n",
    "        load_path = os.path.join(self.load_dir, load_filename)\n",
    "        model_dict = torch.load(load_path)\n",
    "        self.con.load_state_dict(model_dict['Con'])\n",
    "        self.rppg.load_state_dict(model_dict['rPPG'])\n",
    "        self.syn.load_state_dict(model_dict['SGG'])\n",
    "        self.prototype = model_dict['proto'].to(self.device)\n",
    "        self.h = model_dict['h'].to(self.device)\n",
    "        self.c = model_dict['c'].to(self.device)\n",
    "        \n",
    "    # Update learning rate\n",
    "    def update_lr(self, epoch):\n",
    "        self.scheduler_con.step()\n",
    "        self.scheduler_rppg.step()\n",
    "        self.scheduler_syn.step()\n",
    "        self.scheduler_psi.step()\n",
    "        lr = self.optimizer_rppg.param_groups[0]['lr']\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c276fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
